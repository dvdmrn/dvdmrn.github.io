<h1>Conveying Environmental Context While Teleconferencing</h1>

<p><span class="secondary"><b>Collaborators:</b> <i>Pascal Fortin, Max Henry, Jeremy Cooperstock</i></span></p><img
    src="https://www.dvdmrn.xyz/hci/static/media/audio-context-illustration.a87708d5.png" class="project-img">
<p>This project aims to use visual cues to re-introduce ambient contextual awareness that is otherwise lost while
    teleconferencing.</p>
<p>This project translates the user's aural soundscape to visual particles that are animated based off the auditory
    semantics and acoustics of their environment.</p>
<p>Below is a presentation I gave for a WIP of this project at the 2020 Animal Crossing Artificial Intelligence
    Workshop.</p>
<div class="project-video"><iframe class="video" width="560" height="315" src="https://www.youtube.com/embed/wH8sGC4Y9Qw"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"></iframe></div>
<p><b>Publication</b></p>
<ul>
    <li>Marino, D., Henry, M., Fortin, P. E., Bhayana, R., & Cooperstock, J. (2023). I See What You're Hearing: Facilitating The Effect of Environment on Perceived Emotion While Teleconferencing. Proceedings of the ACM on Human-Computer Interaction, 7(CSCW1), 1-15.</li>
</ul>
<p><b>My role on this project</b>:
<ul>
    <li>Project lead</li>
    <li>Study design</li>
    <li>System development (key technologies: Node.js, webrtc, p5.js)</li>
    <li>Contributed to machine learning framework for audio classification</li>
</ul>
</p>